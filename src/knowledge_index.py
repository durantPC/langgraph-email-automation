"""
çŸ¥è¯†åº“ç´¢å¼•æ„å»ºæ¨¡å—
æ”¯æŒè‡ªåŠ¨æ„å»ºå’Œé‡å»ºå‘é‡ç´¢å¼•
"""
import os
import shutil
from typing import List, Optional
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®å¸¸é‡
DATA_DIR = os.getenv("KNOWLEDGE_DATA_DIR", "data")  # çŸ¥è¯†åº“æ–‡æ¡£ç›®å½•
ALLOWED_EXTENSIONS = ['.txt', '.md']  # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼ˆç›®å‰åªæ”¯æŒæ–‡æœ¬æ–‡ä»¶ï¼‰

def get_data_dir() -> str:
    """è·å–æ•°æ®ç›®å½•è·¯å¾„"""
    return DATA_DIR

def get_db_path(embedding_model: Optional[str] = None, api_key: Optional[str] = None) -> str:
    """
    æ ¹æ®åµŒå…¥æ¨¡å‹è‡ªåŠ¨ç¡®å®šæ•°æ®åº“è·¯å¾„
    
    @param embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    @param api_key: APIå¯†é’¥ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    @return: æ•°æ®åº“è·¯å¾„
    """
    if embedding_model is None:
        embedding_model = os.getenv("EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-4B")
    
    if api_key is None:
        api_key = os.getenv("SILICONFLOW_API_KEY")
    
    if not api_key:
        # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        return "db"
    
    try:
        # åˆ›å»ºä¸´æ—¶embeddingsæ¥æ£€æµ‹ç»´åº¦
        embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=api_key,
            openai_api_base="https://api.siliconflow.cn/v1",
            request_timeout=60
        )
        test_vector = embeddings.embed_query("test")
        current_dim = len(test_vector)
        
        # æ ¹æ®ç»´åº¦é€‰æ‹©æ•°æ®åº“ç›®å½•
        if current_dim == 1024:
            return "db_1024"
        elif current_dim == 2560:
            return "db_2560"
        elif current_dim == 4096:
            return "db_4096"
        else:
            return f"db_{current_dim}"
    except Exception as e:
        print(f"âš ï¸ [ç´¢å¼•æ„å»º] æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {e}")
        return "db"

def load_documents_from_dir(data_dir: Optional[str] = None, specific_file: Optional[str] = None) -> List:
    """
    ä»æ•°æ®ç›®å½•åŠ è½½æ‰€æœ‰æ–‡æ¡£æˆ–ç‰¹å®šæ–‡æ¡£
    
    @param data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
    @param specific_file: ç‰¹å®šæ–‡ä»¶åï¼ˆå¦‚æœæŒ‡å®šï¼ŒåªåŠ è½½è¯¥æ–‡ä»¶ï¼‰
    @return: æ–‡æ¡£åˆ—è¡¨
    """
    if data_dir is None:
        data_dir = get_data_dir()
    
    if not os.path.exists(data_dir):
        print(f"âš ï¸ [ç´¢å¼•æ„å»º] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return []
    
    documents = []
    # ä¼˜åŒ–åˆ†å—ç­–ç•¥ï¼šå¢åŠ chunk_sizeä»¥æé«˜ä¸Šä¸‹æ–‡å®Œæ•´æ€§
    # chunk_size=500: æä¾›æ›´å¤šä¸Šä¸‹æ–‡ï¼Œå‡å°‘ä¿¡æ¯æˆªæ–­
    # chunk_overlap=100: å¢åŠ é‡å ï¼Œç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¼šåœ¨åˆ†å—è¾¹ç•Œä¸¢å¤±
    doc_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # ä»300å¢åŠ åˆ°500ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡
        chunk_overlap=100,  # ä»50å¢åŠ åˆ°100ï¼Œç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¸¢å¤±
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]  # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
    )
    
    def load_file_with_encoding(filepath: str, filename: str) -> bool:
        """å°è¯•ä½¿ç”¨å¤šç§ç¼–ç åŠ è½½æ–‡ä»¶"""
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig', 'latin-1']
        
        for encoding in encodings:
            try:
                loader = TextLoader(filepath, encoding=encoding)
                docs = loader.load()
                chunks = doc_splitter.split_documents(docs)
                documents.extend(chunks)
                print(f"âœ… [ç´¢å¼•æ„å»º] åŠ è½½æ–‡ä»¶: {filename}, å—æ•°: {len(chunks)}, ç¼–ç : {encoding}")
                return True
            except UnicodeDecodeError:
                continue
            except Exception as e:
                # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼ˆä¸æ˜¯ç¼–ç é”™è¯¯ï¼‰ï¼Œè®°å½•å¹¶ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªç¼–ç 
                if encoding == encodings[-1]:  # æœ€åä¸€ä¸ªç¼–ç ä¹Ÿå¤±è´¥äº†
                    print(f"âŒ [ç´¢å¼•æ„å»º] åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                    return False
                continue
        
        print(f"âŒ [ç´¢å¼•æ„å»º] åŠ è½½æ–‡ä»¶å¤±è´¥ {filename}: æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶")
        return False
    
    if specific_file:
        # åªåŠ è½½ç‰¹å®šæ–‡ä»¶
        filepath = os.path.join(data_dir, specific_file)
        if not os.path.exists(filepath):
            print(f"âš ï¸ [ç´¢å¼•æ„å»º] æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return []
        
        file_ext = os.path.splitext(specific_file)[1].lower()
        if file_ext not in ALLOWED_EXTENSIONS:
            print(f"âš ï¸ [ç´¢å¼•æ„å»º] ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return []
        
        load_file_with_encoding(filepath, specific_file)
    else:
        # åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        print(f"ğŸ“‚ [ç´¢å¼•æ„å»º] æ‰«æç›®å½•: {data_dir}")
        file_count = 0
        for filename in os.listdir(data_dir):
            filepath = os.path.join(data_dir, filename)
            if not os.path.isfile(filepath):
                print(f"â­ï¸ [ç´¢å¼•æ„å»º] è·³è¿‡éæ–‡ä»¶: {filename}")
                continue
            
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext not in ALLOWED_EXTENSIONS:
                print(f"â­ï¸ [ç´¢å¼•æ„å»º] è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename} (æ‰©å±•å: {file_ext})")
                continue
            
            file_count += 1
            print(f"ğŸ“„ [ç´¢å¼•æ„å»º] å‡†å¤‡åŠ è½½æ–‡ä»¶ ({file_count}): {filename}")
            load_file_with_encoding(filepath, filename)
        
        print(f"ğŸ“Š [ç´¢å¼•æ„å»º] å…±æ‰«æåˆ° {file_count} ä¸ªå¯ç´¢å¼•æ–‡ä»¶")
    
    print(f"ğŸ“Š [ç´¢å¼•æ„å»º] æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£å—")
    return documents

def build_index(
    embedding_model: Optional[str] = None,
    api_key: Optional[str] = None,
    data_dir: Optional[str] = None,
    specific_file: Optional[str] = None,
    db_path: Optional[str] = None
) -> dict:
    """
    æ„å»ºå‘é‡ç´¢å¼•
    
    @param embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    @param api_key: APIå¯†é’¥ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    @param data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
    @param specific_file: ç‰¹å®šæ–‡ä»¶åï¼ˆå¦‚æœæŒ‡å®šï¼Œåªç´¢å¼•è¯¥æ–‡ä»¶ï¼‰
    @param db_path: æ•°æ®åº“è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰
    @return: æ„å»ºç»“æœå­—å…¸
    """
    try:
        # è·å–é…ç½®
        if embedding_model is None:
            embedding_model = os.getenv("EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-4B")
        
        if api_key is None:
            api_key = os.getenv("SILICONFLOW_API_KEY")
        
        if not api_key:
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ° SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡"
            }
        
        # åŠ è½½æ–‡æ¡£
        print(f"ğŸ“š [ç´¢å¼•æ„å»º] å¼€å§‹åŠ è½½æ–‡æ¡£...")
        documents = load_documents_from_dir(data_dir, specific_file)
        
        if not documents:
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°å¯ç´¢å¼•çš„æ–‡æ¡£"
            }
        
        # åˆ›å»ºembeddings
        print(f"ğŸ”§ [ç´¢å¼•æ„å»º] ä½¿ç”¨åµŒå…¥æ¨¡å‹: {embedding_model}")
        try:
            embeddings = OpenAIEmbeddings(
                model=embedding_model,
                openai_api_key=api_key,
                openai_api_base="https://api.siliconflow.cn/v1",
                request_timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            # æµ‹è¯•ç»´åº¦
            test_vector = embeddings.embed_query("test")
            actual_dim = len(test_vector)
            print(f"âœ… [ç´¢å¼•æ„å»º] åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {actual_dim}")
        except Exception as e:
            print(f"âŒ [ç´¢å¼•æ„å»º] ä½¿ç”¨APIåµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            print("   å°è¯•ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹...")
            # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ä½œä¸ºå¤‡ç”¨
            from langchain_community.embeddings import HuggingFaceEmbeddings
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            )
            actual_dim = 384  # æœ¬åœ°æ¨¡å‹çš„é»˜è®¤ç»´åº¦
            print(f"âœ… [ç´¢å¼•æ„å»º] ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼Œç»´åº¦: {actual_dim}")
        
        # ç¡®å®šæ•°æ®åº“è·¯å¾„
        if db_path is None:
            db_path = get_db_path(embedding_model, api_key)
        
        print(f"ğŸ’¾ [ç´¢å¼•æ„å»º] æ•°æ®åº“è·¯å¾„: {db_path}")
        
        # åˆ é™¤æ—§çš„æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(db_path):
            print(f"ğŸ—‘ï¸ [ç´¢å¼•æ„å»º] åˆ é™¤æ—§çš„æ•°æ®åº“: {db_path}")
            try:
                shutil.rmtree(db_path)
                print(f"âœ… [ç´¢å¼•æ„å»º] æ—§æ•°æ®åº“å·²åˆ é™¤")
            except Exception as e:
                print(f"âš ï¸ [ç´¢å¼•æ„å»º] åˆ é™¤æ—§æ•°æ®åº“å¤±è´¥: {e}ï¼Œç»§ç»­å°è¯•...")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        print(f"ğŸš€ [ç´¢å¼•æ„å»º] å¼€å§‹å‘é‡åŒ–æ–‡æ¡£ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰...")
        import time
        start_time = time.time()
        
        try:
            vectorstore = Chroma.from_documents(
                documents=documents,
                embedding=embeddings,
                persist_directory=db_path,
                collection_name="langchain"
            )
            elapsed = time.time() - start_time
            print(f"âœ… [ç´¢å¼•æ„å»º] å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼è€—æ—¶: {elapsed:.1f}ç§’")
            
            return {
                "success": True,
                "message": f"ç´¢å¼•æ„å»ºæˆåŠŸ",
                "db_path": db_path,
                "dimension": actual_dim,
                "chunks": len(documents),
                "elapsed_time": elapsed
            }
        except Exception as e:
            print(f"âŒ [ç´¢å¼•æ„å»º] æ‰¹é‡åˆ›å»ºå¤±è´¥ï¼Œå°è¯•é€ä¸ªæ·»åŠ ...")
            # å°è¯•é€ä¸ªæ·»åŠ æ–‡æ¡£
            try:
                vectorstore = Chroma(
                    persist_directory=db_path, 
                    embedding_function=embeddings,
                    collection_name="langchain"
                )
                for i, doc in enumerate(documents, 1):
                    if i % 10 == 0:
                        print(f"   å¤„ç†è¿›åº¦: {i}/{len(documents)}", end='\r')
                    try:
                        vectorstore.add_documents([doc])
                    except Exception as doc_error:
                        print(f"\n   âš ï¸ æ–‡æ¡£å— {i} å¤„ç†å¤±è´¥: {doc_error}")
                        continue
                
                elapsed = time.time() - start_time
                print(f"\nâœ… [ç´¢å¼•æ„å»º] å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼è€—æ—¶: {elapsed:.1f}ç§’")
                
                return {
                    "success": True,
                    "message": f"ç´¢å¼•æ„å»ºæˆåŠŸï¼ˆé€ä¸ªæ·»åŠ æ¨¡å¼ï¼‰",
                    "db_path": db_path,
                    "dimension": actual_dim,
                    "chunks": len(documents),
                    "elapsed_time": elapsed
                }
            except Exception as sequential_error:
                error_msg = f"ç´¢å¼•æ„å»ºå¤±è´¥: {sequential_error}"
                print(f"âŒ [ç´¢å¼•æ„å»º] {error_msg}")
                return {
                    "success": False,
                    "error": error_msg
                }
    
    except Exception as e:
        error_msg = f"ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
        print(f"âŒ [ç´¢å¼•æ„å»º] {error_msg}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": error_msg
        }

